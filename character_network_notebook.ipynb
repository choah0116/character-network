{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80a6e6e7",
   "metadata": {},
   "source": [
    "# Character Network Analysis Jupyter Notebook\n",
    "This notebook replicates the `characterNetwork-iterative.py` script originally developed by Ken Huang. It uses NLP, sentiment analysis, and graph theory to extract and visualize character relationships from a novel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a04c24",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'networkx'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnetworkx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnx\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'networkx'"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import os\n",
    "import spacy\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from afinn import Afinn\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37aaf264",
   "metadata": {},
   "source": [
    "## Load spaCy Model and Common Words List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd22161",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def load_common_words(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        words = json.load(f)\n",
    "    return set(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9f2489",
   "metadata": {},
   "source": [
    "## Load and Read Novel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b954e42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_novel(book_name, path):\n",
    "    book_list = [i for i in os.listdir(path) if book_name in i]\n",
    "    novel = ''\n",
    "    for i in book_list:\n",
    "        with open(path / i, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            data = f.read().replace('\\r', ' ').replace('\\n', ' ').replace(\"'\", \"'\")\n",
    "            novel += ' ' + data\n",
    "    return novel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7b2683",
   "metadata": {},
   "source": [
    "## Flatten Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbcada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(input_list):\n",
    "    flat_list = []\n",
    "    for i in input_list:\n",
    "        if isinstance(i, list):\n",
    "            flat_list += flatten(i)\n",
    "        else:\n",
    "            flat_list += [i]\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e6f4cb",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0adf71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_entity_recognition(sentence, words):\n",
    "    doc = nlp(sentence)\n",
    "    name_entity = [x for x in doc.ents if x.label_ in ['PERSON', 'ORG']]\n",
    "    name_entity = [str(x).lower().replace(\"'s\", \"\") for x in name_entity]\n",
    "    name_entity = flatten([x.split(' ') for x in name_entity])\n",
    "    name_entity = [x for x in name_entity if len(x) >= 3 and x not in words]\n",
    "    return name_entity\n",
    "\n",
    "def iterative_NER(sentence_list, words, threshold_rate=0.0005):\n",
    "    output = []\n",
    "    for sentence in sentence_list:\n",
    "        names = name_entity_recognition(sentence, words)\n",
    "        if names:\n",
    "            output.append(names)\n",
    "    output = flatten(output)\n",
    "    counts = Counter(output)\n",
    "    return [x for x in counts if counts[x] >= threshold_rate * len(sentence_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd3da78",
   "metadata": {},
   "source": [
    "## Top Character Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabe20d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_names(name_list, novel, top_n=20):\n",
    "    vect = CountVectorizer(vocabulary=name_list, stop_words='english')\n",
    "    freq_matrix = vect.fit_transform([novel.lower()])\n",
    "    freq_df = pd.DataFrame(freq_matrix.toarray(), columns=vect.get_feature_names_out()).T\n",
    "    freq_df.columns = ['count']\n",
    "    freq_df = freq_df.sort_values(by='count', ascending=False).head(top_n)\n",
    "    return freq_df['count'].tolist(), freq_df.index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322a6cc8",
   "metadata": {},
   "source": [
    "## Sentiment Alignment Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1e12a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_align_rate(sentence_list):\n",
    "    afinn = Afinn()\n",
    "    sentiment_score = [afinn.score(x) for x in sentence_list]\n",
    "    nonzero = np.array(sentiment_score)[np.nonzero(sentiment_score)]\n",
    "    return -2 * np.mean(nonzero)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950a1ce7",
   "metadata": {},
   "source": [
    "## Compute Co-occurrence and Sentiment Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea602ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_matrix(name_list, sentence_list, align_rate):\n",
    "    afinn = Afinn()\n",
    "    sentiment_score = np.array([afinn.score(x) for x in sentence_list])\n",
    "    vectorizer = CountVectorizer(vocabulary=name_list, binary=True)\n",
    "    occurrence_matrix = vectorizer.fit_transform(sentence_list).toarray()\n",
    "    co_matrix = np.dot(occurrence_matrix.T, occurrence_matrix)\n",
    "    sent_matrix = np.dot(occurrence_matrix.T, (occurrence_matrix.T * sentiment_score).T)\n",
    "    sent_matrix += align_rate * co_matrix\n",
    "    np.fill_diagonal(co_matrix, 0)\n",
    "    np.fill_diagonal(sent_matrix, 0)\n",
    "    co_matrix = np.tril(co_matrix)\n",
    "    sent_matrix = np.tril(sent_matrix)\n",
    "    return co_matrix, sent_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5551fe3",
   "metadata": {},
   "source": [
    "## Generate Network Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d937d3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_to_edge_list(matrix, mode, name_list):\n",
    "    edge_list = []\n",
    "    shape = matrix.shape[0]\n",
    "    normalized = matrix / np.max(np.abs(matrix)) if np.max(np.abs(matrix)) > 0 else matrix\n",
    "    for i in range(shape):\n",
    "        for j in range(i):\n",
    "            weight = np.log(2000 * normalized[i, j] + 1) * 0.7 if mode == 'co-occurrence' else np.log(abs(1000 * normalized[i, j]) + 1) * 0.7\n",
    "            color = 2000 * normalized[i, j] if mode == 'sentiment' else np.log(2000 * normalized[i, j] + 1)\n",
    "            edge_list.append((name_list[i], name_list[j], {'weight': weight, 'color': color}))\n",
    "    return edge_list\n",
    "\n",
    "def plot_graph(name_list, name_frequency, matrix, title, mode, path=''):\n",
    "    edge_list = matrix_to_edge_list(matrix, mode, name_list)\n",
    "    norm_freq = np.array(name_frequency) / np.max(name_frequency)\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(name_list)\n",
    "    G.add_edges_from(edge_list)\n",
    "    pos = nx.circular_layout(G)\n",
    "    plt.figure(figsize=(14, 14))\n",
    "    edges = G.edges()\n",
    "    weights = [G[u][v]['weight'] for u, v in edges]\n",
    "    colors = [G[u][v]['color'] for u, v in edges]\n",
    "    nx.draw(G, pos, node_color='#A0CBE2', node_size=np.sqrt(norm_freq) * 4000, \n",
    "            edge_color=colors, width=weights, edge_cmap=plt.cm.coolwarm, \n",
    "            with_labels=True, font_size=10)\n",
    "    plt.title(title)\n",
    "    plt.savefig(Path(path) / f\"{title}.png\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dighum101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
